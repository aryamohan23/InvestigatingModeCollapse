# Investigating Mode Collapse in Generative Adversarial Networks

Generative Adversarial Networks (GANs) have seen rapid advancement yet continue to grapple with mode collapse—a phenomenon where the generator produces limited and repetitive samples, diminishing diversity in the generated data. This project investigates mode collapse, examining its implications on the reliability and quality of GAN-generated data. We compare several GAN architectures—standard GAN, Wasserstein GAN with gradient penalty (WGAN-GP), and Unrolled GAN—across different datasets, including synthetic and real-world examples. We assess mode collapse both qualitatively and quantitatively using metrics like the Number of Statistically-Different Bins (NDB score) and Coverage. Our findings reveal Unrolled GAN's superior performance in maintaining data diversity across most datasets. Additionally, our project provides key insights into effective GAN training strategies by emphasizing the need to address catastrophic forgetting and enhance discriminator diversity in order to prevent mode collapse.
